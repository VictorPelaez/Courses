%function [J, grad] = costFunctionReg(theta, X, y, lambda)
%COSTFUNCTIONREG Compute cost and gradient for logistic regression with regularization
%   J = COSTFUNCTIONREG(theta, X, y, lambda) computes the cost of using
%   theta as the parameter for regularized logistic regression and the
%   gradient of the cost w.r.t. to the parameters. 


clear ; close all; clc
data = load('ex2data2.txt');
X = data(:, [1, 2]); y = data(:, 3);
X = mapFeature(X(:,1), X(:,2));


theta=[0.25 0.5 -0.5]';
lambda=0.1;%, X, y, 0.1)
%theta = zeros(size(X, 2), 1);
lambda = 1;

% Initialize some useful values
m = length(y); % number of training examples

% You need to return the following variables correctly 
J = 0;
grad = zeros(size(theta));

% ====================== YOUR CODE HERE ======================
% Instructions: Compute the cost of a particular choice of theta.
%               You should set J to the cost.
%               Compute the partial derivatives and set grad to the partial
%               derivatives of the cost w.r.t. each parameter in theta



n=length(theta);
htheta_final=theta'*X';
htheta_final=sigmoid(htheta_final);
J=(-y'.*log(htheta_final))-((1-y').*log(1-htheta_final));
J=mean(J);
J=J+(lambda/2)*mean(theta(2:n).^2);

grad=sum(repmat((htheta_final-y'),n,1).*X',2)./m;
grad(2:n)=grad(2:n)+(lambda/m).*theta(2:n);


% =============================================================

%end
